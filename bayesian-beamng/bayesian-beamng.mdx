---
title: 'Bayesian Optimization for Vehicle Control'
date: '2024-09-20'
---

I was looking for introductory material on Bayesian optimization and found this helpful paper: ["A Tutorial
on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and
Hierarchical Reinforcement Learning"](http://haikufactory.com/files/bayopt.pdf). Quite a mouthful. In it, 
the authors run an experiment where they train a virtual car to take a clean U-turn at a fixed speed using 
the following control network:

<MdxImage 
    filePath='bayesian-beamng/vehicle_control_network.jpg'
    alt="Control network producing throttle and steering from physical measurements"
    maxWidth="600px"
    originalWidth={1844}
    originalHeight={1290}
    title="Trajectory-following policy"
/>

You can see how, at any given time, the car's throttle and steering are goverened by 4 measurements:

1. The lateral distance between the car and the road, $Y_{err}$
2. The angle between the car's heading and the the road direction, $\Omega_{err}$
3. The difference between the desired speed of the car and the current speed, $V_{err}$
3. The lateral velocity of the car relative to its own heading (a.k.a. "drift"), $V_y$

These are filtered through a total of **15** weights (in combination with summation and tanh squashing) to
produce two values in range (-1, 1). The goal is to come up with a set of weights that encourages the car to stay
on the road without deviating too much from a fixed speed through the duration of the turn.

Let's set up our own simulation and give this a try!

### Background: Bayesian Optimization

So, why do the authors propose this problem as a candidate for Bayesian optimization? Two reasons:

1. We don't know the shape of the reward function a priori. We have to propose a set of weights, send the car
through the U-turn, then calculate the reward at the end of the trial.
2. The reward is expensive to evaluate. We can't afford to probe the entire 15-dimensional parameter space
when each trial requires a real-time simulation.

Bayesian optimization introduces two constructs to address these challenges: the **surrogate function** and
the **acquisition function**. The simplest way to explain their interplay is to walk through a demo of the
optimization of just 1 parameter.

Let's say we've already conducted 5 trials and plotted the reward values:

<MdxImage 
    filePath='bayesian-beamng/bayesopt_demo_points.png'
    alt="Objective values for 5 sample parameter values"
    maxWidth="750px"
    originalWidth={833}
    originalHeight={525}
/>

Where should we conduct the next trial? We're blind to the function that generated these points. However, we can
try to estimate the shape of the function in a manner that leverages the information we've gathered. This is
where we apply a **surrogate function**:

<MdxImage 
    filePath='bayesian-beamng/bayesopt_demo_gp.png'
    alt="Gaussian process for 5 sample parameter values"
    maxWidth="750px"
    originalWidth={833}
    originalHeight={525}
/>

This "function" is actually a **Gaussian Process** - a distribution over infinitely many functions. It's
characterized by its mean (solid line) and uncertainty (transparent envelope). Notice how its mean passes
directly through our observations and how its envelope grows as it steers through uncharted territory. This
reflects our confidence in the estimation of the true function near the observations and our uncertainty
in regions where there is little information upon which to extrapolate. In this example, I've defined it with the 
**Scaled RBF kernel**:

If you'd like to learn exactly how a kernel can define the shape of a GP, I encourage you to visit the paper
that inspired this article.

Now, we have to decide where to conduct our next trial based on the surrogate. It predicts a low-uncertainty
peak near the rightmost observation. However, it appears that the high-uncertainty region on the left 
has the potential to yield an even greater reward. We need a mechanism that can incorporate both of these 
considerations to estimate the improvement we can expect to observe at any point in the domain. This
is where an **acquisition function** can help. Here, I've chosen to implement the aptly-named
**Expected Improvement** function:

<MdxImage 
    filePath='bayesian-beamng/bayesopt_demo_acq.png'
    alt="Gaussian process and acquisition function for 5 sample parameter values"
    maxWidth="750px"
    originalWidth={902}
    originalHeight={525}
/>

You'll notice that this expression is a sum of two terms. The first is the **Exploitation Term**. It says:
"look for points where the mean of the GP is greater than the best observation with little uncertainty". This
is the main contribution to the peak on the right. The second term is the **Exploration Term**. It says:
"look for points where large uncertainty permits a high probability of exceeding the best observation". This
drives the peak on the left. Also, note the tradeoff parameter, $\gamma$. This allows us to adjust the balance
of the terms to create a more/less adventurous optimizer.

My choice of $\gamma$ has resulted in a peak around $x=3$, so this is where we would conduct the next trial
and redo this process with 6 observations.

### Configuring the Simulation

In order to conduct optimization, we need to simulate a car. You may have heard of the
impressively-realistic vehicle physics simulator, [BeamNG.drive](https://www.beamng.com/game/). You may not 
be familiar with its research-oriented fork, [BeamNG.tech](https://beamng.tech). It comes with a host of neat 
features that make experimentation easier, but the one that will come in handy today is the Python API, 
BeamNGpy. Props to the developers for rescuing me from having to hack together LUA scripts.

[INSERT BEAMNG.TECH VISUAL]

Let's go over the general plan. BeamNG allows us to poll the position, velocity, and orientation of the car
as 3D Cartesian vectors (though we will discard $\hat{z}$). There is an implicit global origin and fixed basis. 
We need to transform these into the 4 measurements that will feed into our control network. Two of the 
transformations are straightforward:

* $V_{err}$: Subtract the magnitude of the velocity vector from the desired speed.
* $V_y$ (a.k.a. "drift"): Project the velocity vector onto the lateral component of the orientation vector.

$Y_{err}$ and $\Omega_{err}$ are a bit more complicated. I propose the following construct to streamline
their measurement: a sliding origin, $A$.

[INSERT SLIDING ORIGIN IMAGE]

$A$ will be constrained to the U-turn's line of symmetry. It will follow the car up along $\hat{y}$, stop at
the center of the U-turn, then follow the car back down the final straight. This gives us a vector between the 
origin and the car, $\vec{AC}$, that facilitates the final 2 measurements:

* $Y_{err}$: Subtract the radius of the curve from $|\vec{AC}|$.
* $\Omega_{err}$: Rotate $\vec{AC}$ $\pi/2$ clockwise to get the car's *ideal* heading. Calculate the angle
between the car's orientation and the ideal heading.